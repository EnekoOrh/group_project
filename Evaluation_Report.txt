PROJECT EVALUATION AND SANITY CHECK
-------------------------------------

1. ASSIGNMENT INTERPRETATION
----------------------------
Based on the provided PDF content, the core objective of Task 3 is not merely to "code" an optimizer, but to produce a comparative scientific study of two specific algorithms: Simulated Annealing (SA) and Particle Swarm Optimization (PSO).

The assignment explicitly asks for:
- Investigation of unconstrained and constrained techniques.
- Three specific optimization problems (excluding Cooling Tower and Griewank).
- A comparison of performance using "Number of Evaluations" as a primary metric, based on the assumption that objective function calls are expensive.
- Sensitivity analysis of the Penalty Function factor for the constrained problem.
- Graphical representations of results.

Implicit Expectations:
- "Apple-to-Apples" Comparison: In academia, comparing Algorithm A (running for 500 evaluations) against Algorithm B (running for 15,000 evaluations) is considered invalid unless clearly normalized.
- Stochastic Confidence: Since these are random algorithms, running them once is insufficient. You must run them multiple times (e.g., 30 runs) and report Mean and Standard Deviation (which your code does).
- Visual Proof: Convergence plots and Surface plots are standard tools to demonstrate "understanding" of the landscape.

2. CODE EVALUATION
------------------
Your codebase is currently at a "Solid Pass" level, with potential for "Merit/Distinction" with minor tweaks.

Strong Points:
- Architecture: The Object-Oriented structure (OptimizationAlgorithm base class) is clean, extensible, and correct. This is good software engineering practice.
- Reproducibility: You correctly handle random seeds (`seed=i`), ensuring your results can be verified.
- Statistical Rigor: You are running 30 trials and calculating Mean/Std Dev. This is exactly what is expected for a stochastic optimization study.
- Visualization: Your `plot_surfaces.py` script is excellent. Generating 3D surfaces of the test functions provides high-quality visual filler for the report that demonstrates the complexity of the problems.

Weak Points:
- Comparison Logic (The "Fairness" Issue): This is the biggest academic weakness. Your code runs both algorithms for the same `max_iter` (default 500).
    - SA: 1 evaluation per iteration = 500 total.
    - PSO: 30 particles per iteration = 15,000 total.
    - Result: You are comparing a sprinter who ran 500m to a marathon runner who ran 15km. Of course, PSO finds a better answer; it did 30x the work.

3. IDENTIFIED MISALIGNMENTS
---------------------------
Strict Academic Evaluator Feedback:

A. The "Evaluations" Metric:
The assignment specifically highlights: "count the number of evaluations... on the assumption that... each such evaluation is expensive."
Your current code tracks `n_evals`, but you plot convergence based on `Iterations`.
- Misalignment: A graph showing PSO dropping faster vs "Iterations" is misleading because one PSO iteration costs 30x more than one SA iteration.
- Correction: The X-axis of your convergence plots MUST be "Number of Evaluations".
- If you plot vs Evaluations, SA will look much more competitive (it drops early), whereas PSO might look slower initially but deeper eventually. This is the "correct" scientific insight.

B. Problem Selection:
You chose: 1. Rastrigin, 2. Rosenbrock, 3. Constrained Rosenbrock.
- Critique: While "technically" three problems, reusing Rosenbrock twice covers less breadth.
- Suggestion: The assignment asks for "three optimisation problems". It is safer to choose three *functionally distinct* landscapes (e.g., Rastrigin, Rosenbrock, and Sphere/Ackley) AND apply constraints to one of them. However, your current selection is likely acceptable if time is short.

4. EVALUATION OF POTENTIAL UPGRADES
-----------------------------------
To move this from a "Good" project to an "Excellent" one:

1. Normalize the Comparison (CRITICAL):
   Modify `run_experiments.py` to allow running by `max_evals` budget.
   - Set Budget = 10,000 evaluations.
   - SA runs for 10,000 iterations.
   - PSO runs for 333 iterations (30 particles).
   - THEN compare the final values. This is the only scientifically valid way to answer "which algorithm is better?".

2. Fix the Convergence Plots:
   In `plot_convergence`, change the X-axis from:
   `iterations = np.arange(len(mean_sa))`
   to:
   `evals_sa = np.arange(len(mean_sa)) * 1`
   `evals_pso = np.arange(len(mean_pso)) * 30` (assuming 30 particles)
   This will show the true cost of convergence.

3. Enhance the Constraint Study:
   Currently, the Constrained Rosenbrock minimum (1,1) lies exactly on the constraint boundary ($1^2+1^2=2$).
   - Upgrade: Use a constraint that forces the solution *away* from the unconstrained global minimum.
   - Example: Rosenbrock Global Min is (1,1). Set constraint $x^2 + y^2 <= 0.5$. Now the algorithm cannot just go to (1,1). It must find the "best possible" point inside the small circle. This demonstrates the conflict between Objective and Constraint much better.

5. FINAL SANITY CHECK SUMMARY
-----------------------------
Are we fulfilling the assignment?
YES. You have all the required components: algorithms, problems, metrics, and report structure.

What MUST be improved before submission:
- The "Unfair Comparison" in your results. You must either run them with equal evaluation budgets OR explicitly explain in your report that "PSO performed better but used 30x more resources," and prove this by fixing your convergence plots to use "Evaluations" on the x-axis.

So, the immediate action list:
1. Update `plot_convergence` to use evaluations on X-axis.
2. (Optional but recommended) Run a "Budget-Based" experiment where SA gets 30x more iterations to give it a fighting chance.

Verdict: Code is High Quality, Experimental Design needs specific rigour adjustment.
